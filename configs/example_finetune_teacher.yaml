### General ###
distill: False
result_path: "./result/docvqa"
verbose: True
log_interval: 10 # After how many steps the logger should log
use_student_without_distilling: False # When finetuning student
wandb_name: "Finetune Teacher"

### transformer parameters ###
model_id: 'naver-clova-ix/donut-base'
max_length: 128
input_size: [1280, 960] # when the input resolution differs from the pre-training setting, some weights will be newly initialized (but the model training would be okay)

### Dataset parameters ###
dataset: "./preprocessed_dataset_docvqa/" # TODO: Change if necessary
dataset_name_training: "train"
dataset_name_validate: "validation"
sort_json_key: False
align_long_axis: False

### Train parameters ###
train_batch_sizes: 4
accumulation_steps: 1
lr: !!float 3e-5
gradient_clip_val: 0.25

num_nodes: 1
num_workers: 5

warmup_steps: 10000 # 800/8*30/10, 10%
max_epochs: 30
max_steps: -1

### Validation parameters ###
val_batch_sizes: 1
val_check_interval: 0.2
limit_val_batches: 1

